# üß† MODEL_INFO.md

## üìå Objectif

Ce fichier documente les mod√®les utilis√©s dans le projet **RAG AI Assistant**, leur r√¥le, leur fonctionnement et pourquoi ils ont √©t√© choisis.

---

## üîπ 1. Sentence Transformer

- **Mod√®le utilis√©** : `all-MiniLM-L6-v2`
- **Biblioth√®que** : [`sentence-transformers`](https://www.sbert.net/)
- **Fonction** : Convertir chaque chunk de texte issu des PDF en vecteurs num√©riques pour permettre la recherche s√©mantique.
- **Utilisation dans le code** :

```python
from sentence_transformers import SentenceTransformer
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
vectors = embedding_model.encode(text_chunks)
```

## üîπ 2. FAISS (Facebook AI Similarity Search)

R√¥le : Stocker et rechercher rapidement les vecteurs de texte pour retrouver les documents les plus proches d'une question.
Type d'index utilis√© : IndexFlatL2

```python
import faiss
index = faiss.IndexFlatL2(dimension)
index.add(vectors)
```
- **Pourquoi FAISS ?**
    Ultra rapide en recherche de similarit√©
    Maintenu par Facebook AI Research
    Parfait pour du RAG local

## üîπ 3. LLM Local (ex: LLaMA 3 via Ollama)

- **Mod√®le appel√©** : llama3
- **Serveur d'inf√©rence** : API locale disponible √† http://localhost:11434/api/generate (via Ollama)
- **M√©thode d'appel dans le code** :

```python
response = requests.post(
    "http://localhost:11434/api/generate",
    json={"model": "llama3", "prompt": prompt, "stream": False}
)
```
- **Prompt utilis√©** :
    Tu es un assistant expert. R√©ponds √† la question uniquement √† partir du contexte fourni.

    Contexte :
    ...

    Question : ...
    R√©ponse :

- **Pourquoi ce choix ?**
    Permet de travailler sans envoyer les donn√©es sur internet
    Pr√©cis, fluide, personnalisable
    Facilement rempla√ßable par d'autres mod√®les compatibles

## üß© Interaction entre les mod√®les
1. SentenceTransformer ‚Üí encode les morceaux de texte
2. FAISS ‚Üí retrouve les chunks les plus proches d‚Äôune question
3. LLM (LLaMA 3) ‚Üí g√©n√®re la r√©ponse en se basant uniquement sur les chunks r√©cup√©r√©s


[PDFs] ‚Üí [Texte] ‚Üí [Chunks] ‚Üí [Embeddings] ‚Üí [FAISS_Index]
                                                   ‚Üì
                                               [Question]
                                                   ‚Üì
                                        [Top3_Chunks_similaires]
                                                   ‚Üì
                                        [Prompt_envoy√©_au_LLM]
                                                   ‚Üì
                                              [R√©ponse_finale]
